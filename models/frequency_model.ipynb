{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-14T19:43:55.966546Z","iopub.status.busy":"2024-06-14T19:43:55.965434Z","iopub.status.idle":"2024-06-14T19:43:55.982304Z","shell.execute_reply":"2024-06-14T19:43:55.980977Z","shell.execute_reply.started":"2024-06-14T19:43:55.966497Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:43:55.985081Z","iopub.status.busy":"2024-06-14T19:43:55.984629Z","iopub.status.idle":"2024-06-14T19:43:55.990291Z","shell.execute_reply":"2024-06-14T19:43:55.989158Z","shell.execute_reply.started":"2024-06-14T19:43:55.985033Z"},"trusted":true},"outputs":[],"source":["file_path = '/kaggle/input/stanford-earthquake-dataset-stead/merge.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:43:55.991976Z","iopub.status.busy":"2024-06-14T19:43:55.991616Z","iopub.status.idle":"2024-06-14T19:44:26.464860Z","shell.execute_reply":"2024-06-14T19:44:26.462846Z","shell.execute_reply.started":"2024-06-14T19:43:55.991946Z"},"trusted":true},"outputs":[],"source":["!pip install basemap\n","!pip install basemap-data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:44:26.467640Z","iopub.status.busy":"2024-06-14T19:44:26.467219Z","iopub.status.idle":"2024-06-14T19:44:45.350779Z","shell.execute_reply":"2024-06-14T19:44:45.349297Z","shell.execute_reply.started":"2024-06-14T19:44:26.467604Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv(file_path, low_memory=False)\n","eqdf = df[df.source_magnitude > 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:44:45.355624Z","iopub.status.busy":"2024-06-14T19:44:45.355171Z","iopub.status.idle":"2024-06-14T19:44:46.973416Z","shell.execute_reply":"2024-06-14T19:44:46.972260Z","shell.execute_reply.started":"2024-06-14T19:44:45.355590Z"},"trusted":true},"outputs":[],"source":["eqdf.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:44:46.975090Z","iopub.status.busy":"2024-06-14T19:44:46.974770Z","iopub.status.idle":"2024-06-14T19:45:10.312165Z","shell.execute_reply":"2024-06-14T19:45:10.310926Z","shell.execute_reply.started":"2024-06-14T19:44:46.975061Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","longitude = eqdf['source_longitude']\n","latitude = eqdf['source_latitude']\n","magnitude = eqdf['source_magnitude']\n","\n","plt.figure(figsize=(10, 6))\n","sc = plt.scatter(longitude, latitude, c=magnitude, cmap='viridis', alpha=0.5)\n","plt.colorbar(sc, label='Magnitude')\n","plt.xlabel('Longitude')\n","plt.ylabel('Latitude')\n","plt.title('Heatmap of Earthquake Magnitudes')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:45:10.314318Z","iopub.status.busy":"2024-06-14T19:45:10.313876Z","iopub.status.idle":"2024-06-14T19:45:10.996789Z","shell.execute_reply":"2024-06-14T19:45:10.995661Z","shell.execute_reply.started":"2024-06-14T19:45:10.314281Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from sklearn.neighbors import NearestNeighbors\n","import numpy as np\n","\n","def haversine(lat1, lon1, lat2, lon2):\n","    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n","    dlat = lat2 - lat1\n","    dlon = lon2 - lon1\n","    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n","    c = 2 * np.arcsin(np.sqrt(a))\n","    r = 6371\n","    return c * r\n","\n","def find_nearest_earthquake(eqdf, lat, lon):\n","    nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n","    nbrs.fit(eqdf[['source_latitude', 'source_longitude']])\n","\n","    input_point = np.array([[lat, lon]])\n","\n","    distances, indices = nbrs.kneighbors(input_point)\n","    nearest_earthquake = eqdf.iloc[indices[0][0]]\n","\n","    haversine_distance = haversine(lat, lon, nearest_earthquake['source_latitude'], nearest_earthquake['source_longitude'])\n","    \n","    return nearest_earthquake, haversine_distance\n","\n","earthquake_data, distance = find_nearest_earthquake(eqdf, 45.00, 20.10)\n","print(f\"Nearest earthquake details: \\n{earthquake_data}\")\n","print(f\"Haversine distance to nearest earthquake: {distance} km\")"]},{"cell_type":"markdown","metadata":{},"source":["# FREQUENCIES"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:45:10.998600Z","iopub.status.busy":"2024-06-14T19:45:10.998244Z","iopub.status.idle":"2024-06-14T19:45:28.027295Z","shell.execute_reply":"2024-06-14T19:45:28.026013Z","shell.execute_reply.started":"2024-06-14T19:45:10.998571Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv(file_path, low_memory=False)\n","src_df = df[df.source_distance_km > 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:45:28.029535Z","iopub.status.busy":"2024-06-14T19:45:28.029129Z","iopub.status.idle":"2024-06-14T19:45:28.036947Z","shell.execute_reply":"2024-06-14T19:45:28.035754Z","shell.execute_reply.started":"2024-06-14T19:45:28.029503Z"},"trusted":true},"outputs":[],"source":["src_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:45:28.039692Z","iopub.status.busy":"2024-06-14T19:45:28.038619Z","iopub.status.idle":"2024-06-14T19:45:43.102519Z","shell.execute_reply":"2024-06-14T19:45:43.101012Z","shell.execute_reply.started":"2024-06-14T19:45:28.039649Z"},"trusted":true},"outputs":[],"source":["!pip install h5py tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:45:43.105612Z","iopub.status.busy":"2024-06-14T19:45:43.105057Z","iopub.status.idle":"2024-06-14T19:45:47.068948Z","shell.execute_reply":"2024-06-14T19:45:47.067650Z","shell.execute_reply.started":"2024-06-14T19:45:43.105560Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt\n","from scipy.signal import find_peaks\n","\n","ev_list = src_df['trace_name'].to_list()\n","file_name = \"/kaggle/input/stanford-earthquake-dataset-stead/merge.hdf5\"\n","dtfl = h5py.File(file_name, 'r')\n","\n","for c, evi in enumerate(ev_list):\n","    dataset = dtfl.get('data/'+str(evi))\n","    if dataset:\n","        data = np.array(dataset)\n","        fft_data = np.fft.fft(data, axis=0)\n","        freq = np.fft.fftfreq(data.shape[0])\n","\n","        positive_freq_indices = freq > 0\n","        positive_freqs = freq[positive_freq_indices]\n","        positive_fft_data = fft_data[positive_freq_indices]\n","\n","        fig = plt.figure(figsize=(24, 18))\n","\n","        all_peak_freqs = []  \n","\n","        for i in range(3):  # E, N, Z channels\n","            ax = fig.add_subplot(3, 3, 3*i + 1)\n","            plt.plot(data[:, i], 'k')\n","            plt.title(f\"Channel {i+1} Time Domain\")\n","            plt.ylabel('Amplitude')\n","            plt.xlabel('Time')\n","            plt.tight_layout()\n","\n","        max_freq = positive_freqs[-1]\n","        for i in range(3):\n","            ax = fig.add_subplot(3, 3, 3*i + 2)\n","            plt.plot(positive_freqs, np.abs(positive_fft_data[:, i]), 'r') \n","            plt.xlim(0, max_freq)\n","            plt.title(f\"Channel {i+1} Frequency Domain\")\n","            plt.xlabel('Frequency (Hz)')\n","            plt.ylabel('Magnitude')\n","            plt.tight_layout()\n","            \n","        threshold = 0.8\n","        for i in range(3):\n","            ax = fig.add_subplot(3, 3, 3*i + 3)\n","            magnitude = np.abs(positive_fft_data[:, i])\n","            peaks, _ = find_peaks(magnitude, height=np.max(magnitude) * threshold)\n","            peak_freqs = positive_freqs[peaks]\n","            all_peak_freqs.append(peak_freqs)\n","            plt.vlines(peak_freqs, ymin=0, ymax=magnitude[peaks], color='b')\n","            plt.xlim(0, max_freq)\n","            plt.title(f\"Channel {i+1} Peaks Only\")\n","            plt.xlabel('Frequency (Hz)')\n","            plt.ylabel('Magnitude')\n","            plt.tight_layout()\n","\n","        plt.show()\n","\n","        for i, freqs in enumerate(all_peak_freqs):\n","            print(f\"Peak frequencies for Channel {i+1}: {freqs}\")\n","\n","        print(f\"Metadata for {evi}:\")\n","        for key, value in dataset.attrs.items():\n","            print(f\"{key}: {value}\")\n","\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:45:47.071177Z","iopub.status.busy":"2024-06-14T19:45:47.070769Z","iopub.status.idle":"2024-06-14T19:52:10.751010Z","shell.execute_reply":"2024-06-14T19:52:10.749806Z","shell.execute_reply.started":"2024-06-14T19:45:47.071143Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import h5py\n","from scipy.signal import find_peaks\n","from tqdm import tqdm\n","\n","file_name = \"/kaggle/input/stanford-earthquake-dataset-stead/merge.hdf5\"\n","dtfl = h5py.File(file_name, 'r')\n","features = []\n","labels = []\n","\n","idx = 0\n","\n","for evi in tqdm(src_df['trace_name'].to_list(), total=10000):\n","    idx += 1\n","    dataset = dtfl.get('data/'+str(evi))\n","    if dataset:\n","        source_distance = dataset.attrs.get('source_distance_km', None)\n","        source_magnitude = dataset.attrs.get('source_magnitude', None)\n","        source_depth = dataset.attrs.get('source_depth_km', None)\n","\n","        if source_distance in [None, 'None'] or source_magnitude in [None, 'None'] or source_depth in [None, 'None']:\n","            continue\n","\n","        complete_feature_set = [\n","            float(source_distance),\n","            float(source_magnitude),\n","            float(source_depth)\n","        ]\n","\n","        data = np.array(dataset)\n","        fft_data = np.fft.fft(data, axis=0)\n","        freq = np.fft.fftfreq(data.shape[0])\n","\n","        positive_freq_indices = freq > 0\n","        positive_freqs = freq[positive_freq_indices]\n","        positive_fft_data = fft_data[positive_freq_indices]\n","\n","        weighted_frequencies = []\n","\n","        for i in range(3):\n","            fft_magnitude = np.abs(positive_fft_data[:, i])\n","            peaks, properties = find_peaks(fft_magnitude, height=np.max(fft_magnitude) * 0.9)\n","            if len(peaks) > 0:\n","                peak_freqs = positive_freqs[peaks]\n","                peak_mags = properties['peak_heights']\n","                weighted_avg = np.average(peak_freqs, weights=peak_mags)\n","                weighted_frequencies.append(weighted_avg)\n","            else:\n","                weighted_frequencies.append(0)\n","\n","        features.append(complete_feature_set)\n","        labels.append([round(freq, 3) for freq in weighted_frequencies])\n","    \n","    if idx > 9999:\n","        break\n","\n","features = np.array(features, dtype=float)\n","labels = np.array(labels, dtype=float)\n","\n","print(\"Features example:\", features[:10])\n","print(\"Labels example:\", labels[:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:52:10.752906Z","iopub.status.busy":"2024-06-14T19:52:10.752570Z","iopub.status.idle":"2024-06-14T19:52:27.992889Z","shell.execute_reply":"2024-06-14T19:52:27.991575Z","shell.execute_reply.started":"2024-06-14T19:52:10.752877Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","\n","fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n","\n","lower_percentile = 10\n","upper_percentile = 99.5\n","intervals = 75\n","\n","for i in range(3):\n","    for j in range(3):\n","        axs[i, j].scatter(features[:, j], labels[:, i], label=f'Label {i}', alpha=0.5, color='grey')\n","\n","        min_value = np.min(features[:, j])\n","        max_value = np.max(features[:, j])\n","        range_width = (max_value - min_value) / intervals\n","\n","        for section in range(intervals):\n","            section_min = min_value + section * range_width\n","            section_max = section_min + range_width\n","            \n","            section_indices = (features[:, j] >= section_min) & (features[:, j] < section_max)\n","            section_features = features[section_indices, j]\n","            section_labels = labels[section_indices, i]\n","\n","            if len(section_labels) > 0:\n","                low_limit = np.percentile(section_labels, lower_percentile)\n","                high_limit = np.percentile(section_labels, upper_percentile)\n","                clean_indices = (section_labels > low_limit) & (section_labels < high_limit)\n","                clean_features = section_features[clean_indices]\n","                clean_labels = section_labels[clean_indices]\n","\n","                axs[i, j].scatter(clean_features, clean_labels, color='blue', alpha=0.5)\n","\n","                if len(clean_labels) > 0:\n","                    top_indices = np.argsort(clean_labels)[-1:]\n","                    top_features = clean_features[top_indices]\n","                    top_labels = clean_labels[top_indices]\n","\n","                    axs[i, j].scatter(top_features, top_labels, color='red', s=50)\n","\n","        axs[i, j].set_xlabel(f'Feature {j} Value')\n","        axs[i, j].set_ylabel(f'Label {i} Value')\n","        axs[i, j].legend()\n","        axs[i, j].grid(True)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:52:27.998916Z","iopub.status.busy":"2024-06-14T19:52:27.998424Z","iopub.status.idle":"2024-06-14T19:52:51.541701Z","shell.execute_reply":"2024-06-14T19:52:51.540426Z","shell.execute_reply.started":"2024-06-14T19:52:27.998875Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.optimize import curve_fit\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","def exponential_decreasing(x, a, b):\n","    return a * np.exp(-b * x)\n","\n","def build_model():\n","    model = models.Sequential([\n","        layers.Dense(64, activation='relu', input_shape=(1,)),\n","        layers.Dense(64, activation='relu'),\n","        layers.Dense(1)\n","    ])\n","    model.compile(optimizer='adam', loss='mse')\n","    return model\n","\n","num_top_values = 3\n","start_value = 20\n","end_value = np.max(features[:, 0])\n","\n","fig, axs = plt.subplots(1, 3, figsize=(30, 10))\n","models = [build_model() for _ in range(3)]  # Create a separate model for each label\n","\n","for label_index in range(3):\n","    top_features = []\n","    top_labels = []\n","\n","    valid_indices = features[:, 0] >= start_value\n","    valid_features = features[valid_indices]\n","    valid_labels = labels[valid_indices]\n","\n","    feature_index = 0\n","\n","    axs[label_index].scatter(valid_features[:, feature_index], valid_labels[:, label_index], label=f'Label {label_index}', alpha=0.5, color='grey')\n","\n","    min_value = np.min(valid_features[:, feature_index])\n","    max_value = np.max(valid_features[:, feature_index])\n","    range_width = (max_value - min_value) / intervals\n","\n","    for section in range(intervals):\n","        section_min = min_value + section * range_width\n","        section_max = section_min + range_width\n","        \n","        section_indices = (valid_features[:, feature_index] >= section_min) & (valid_features[:, feature_index] < section_max)\n","        section_features = valid_features[section_indices, feature_index]\n","        section_labels = valid_labels[section_indices, label_index]\n","\n","        if len(section_labels) > 0:\n","            low_limit = np.percentile(section_labels, lower_percentile)\n","            high_limit = np.percentile(section_labels, upper_percentile)\n","            clean_indices = (section_labels > low_limit) & (section_labels < high_limit)\n","            clean_features = section_features[clean_indices]\n","            clean_labels = section_labels[clean_indices]\n","\n","            axs[label_index].scatter(clean_features, clean_labels, color='blue', alpha=0.5)\n","\n","            if len(clean_labels) > 0:\n","                top_indices = np.argsort(clean_labels)[-num_top_values:]\n","                top_features.extend(clean_features[top_indices])\n","                top_labels.extend(clean_labels[top_indices])\n","\n","    axs[label_index].scatter(top_features, top_labels, color='red', s=50)\n","\n","    axs[label_index].set_xlabel(f'Feature {feature_index} Value')\n","    axs[label_index].set_ylabel(f'Label {label_index} Value')\n","    axs[label_index].legend()\n","    axs[label_index].grid(True)\n","\n","    if len(top_features) > 0:\n","        top_features = np.array(top_features)\n","        top_labels = np.array(top_labels)\n","\n","        log_labels = np.log(top_labels)\n","        \n","        models[label_index].fit(top_features[:, np.newaxis], log_labels, epochs=150, verbose=0)\n","        \n","        x_fit = np.linspace(start_value, end_value, 100)\n","        log_y_fit = models[label_index].predict(x_fit[:, np.newaxis]).flatten()\n","        y_fit = np.exp(log_y_fit)\n","        \n","        axs[label_index].plot(x_fit, y_fit, color='green', label='NN Exponential Fit for Top Values')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","def predict_x(input_feature):\n","    log_y = models[0].predict(np.array([[input_feature]]))\n","    return np.exp(log_y.flatten())\n","\n","def predict_y(input_feature):\n","    log_y = models[1].predict(np.array([[input_feature]]))\n","    return np.exp(log_y.flatten())\n","\n","def predict_z(input_feature):\n","    log_y = models[2].predict(np.array([[input_feature]]))\n","    return np.exp(log_y.flatten())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:52:51.543686Z","iopub.status.busy":"2024-06-14T19:52:51.543268Z","iopub.status.idle":"2024-06-14T19:52:51.879278Z","shell.execute_reply":"2024-06-14T19:52:51.878124Z","shell.execute_reply.started":"2024-06-14T19:52:51.543648Z"},"trusted":true},"outputs":[],"source":["print(predict_x(250))\n","print(predict_y(25))\n","print(predict_z(25))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:59:49.471320Z","iopub.status.busy":"2024-06-14T19:59:49.470784Z","iopub.status.idle":"2024-06-14T19:59:50.350416Z","shell.execute_reply":"2024-06-14T19:59:50.349506Z","shell.execute_reply.started":"2024-06-14T19:59:49.471281Z"},"trusted":true},"outputs":[],"source":["in_lat = 45.00\n","in_lon = 20.10\n","\n","earthquake_data, distance_km = find_nearest_earthquake(eqdf, in_lat, in_lon)\n","x_freq = predict_x(distance_km)\n","y_freq = predict_y(distance_km)\n","z_freq = predict_z(distance_km)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-14T19:59:58.636007Z","iopub.status.busy":"2024-06-14T19:59:58.635012Z","iopub.status.idle":"2024-06-14T19:59:58.642623Z","shell.execute_reply":"2024-06-14T19:59:58.641354Z","shell.execute_reply.started":"2024-06-14T19:59:58.635971Z"},"trusted":true},"outputs":[],"source":["print(x_freq, y_freq, z_freq)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1811884,"sourceId":2955216,"sourceType":"datasetVersion"}],"dockerImageVersionId":30715,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
