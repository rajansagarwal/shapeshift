{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8jGYgbGyeNM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kWEMoZp8oLo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import folium\n",
        "from folium import Choropleth\n",
        "from folium.plugins import HeatMap\n",
        "import datetime\n",
        "\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ewoT7BE8sJW"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"df.csv\")\n",
        "\n",
        "missing_values_columns = [col for col in data.columns\n",
        "                     if data[col].isnull().any()]\n",
        "data = data.drop(missing_values_columns, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ntd4fHEs9abd"
      },
      "outputs": [],
      "source": [
        "tectonic_plates = pd.read_csv(\"datasets/all.csv\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeYljq4F9wFn"
      },
      "outputs": [],
      "source": [
        "lengths = data[\"Date\"].str.len()\n",
        "lengths.value_counts()\n",
        "\n",
        "wrongdates = np.where([lengths == 24])[1]\n",
        "data.loc[3378, \"Date\"] = \"02/23/1975\"\n",
        "data.loc[7512, \"Date\"] = \"04/28/1985\"\n",
        "data.loc[20650, \"Date\"] = \"03/13/2011\"\n",
        "data.loc[3378, \"Time\"] = \"02:58:41\"\n",
        "data.loc[7512, \"Time\"] = \"02:53:41\"\n",
        "data.loc[20650, \"Time\"] = \"02:23:34\"\n",
        "data['Date']= pd.to_datetime(data[\"Date\"])\n",
        "\n",
        "lengths = data[\"Time\"].str.len()\n",
        "lengths.value_counts()\n",
        "data['Time']= pd.to_timedelta(data['Time'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwWavqEY-MM2"
      },
      "outputs": [],
      "source": [
        "data[\"Date_Time\"]=data[\"Date\"] +data[\"Time\"]\n",
        "data[\"Days\"]= data.Date.dt.strftime(\"%A\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5okNKG7-Nw_"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 3))\n",
        "ts = sns.lineplot(x=data[\"Date\"].dt.year, y=\"Magnitude\", data=data, color=\"#732e75\")\n",
        "ts.set_title(\"Earthquake Time Series\", color=\"#18508d\")\n",
        "ts.set_ylabel(\"Magnitude\", color=\"#58508d\")\n",
        "ts.set_xlabel(\"Date\", color=\"#58508d\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D-mlTvl-nmn"
      },
      "outputs": [],
      "source": [
        "colours = [\"#732e75\",\"#bc5090\",\"#ff6361\",\"#003f5c\"]\n",
        "depth = data[\"Depth\"].values\n",
        "mean_depth= data[\"Depth\"].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lIl1lCG-6hm"
      },
      "outputs": [],
      "source": [
        "tectonic = folium.Map(tiles=\"cartodbpositron\", zoom_start=5)\n",
        "\n",
        "plates = list(tectonic_plates[\"plate\"].unique())\n",
        "for plate in plates:\n",
        "    plate_vals = tectonic_plates[tectonic_plates[\"plate\"] == plate]\n",
        "    lats = plate_vals[\"lat\"].values\n",
        "    lons = plate_vals[\"lon\"].values\n",
        "    points = list(zip(lats, lons))\n",
        "    indexes = [None] + [i + 1 for i, x in enumerate(points) if i < len(points) - 1 and abs(x[1] - points[i + 1][1]) > 300] + [None]\n",
        "    for i in range(len(indexes) - 1):\n",
        "        folium.vector_layers.PolyLine(points[indexes[i]:indexes[i+1]], popup=plate, color=\"#58508d\", fill=False, ).add_to(tectonic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLDJCQhW_cEW"
      },
      "outputs": [],
      "source": [
        "from branca.element import Figure\n",
        "fig = Figure(width=600, height=400)\n",
        "\n",
        "tectonic_quake = folium.Map(tiles=\"cartodbpositron\", zoom_start=5)\n",
        "gradient = {.33: \"#58508d\", .66: \"#ef5675\", 1: \"#ffa600\"}\n",
        "plates = list(tectonic_plates[\"plate\"].unique())\n",
        "for plate in plates:\n",
        "    plate_vals = tectonic_plates[tectonic_plates[\"plate\"] == plate]\n",
        "    lats = plate_vals[\"lat\"].values\n",
        "    lons = plate_vals[\"lon\"].values\n",
        "    points = list(zip(lats, lons))\n",
        "    indexes = [None] + [i + 1 for i, x in enumerate(points) if i < len(points) - 1 and abs(x[1] - points[i + 1][1]) > 300] + [None]\n",
        "    for i in range(len(indexes) - 1):\n",
        "        folium.vector_layers.PolyLine(points[indexes[i]:indexes[i+1]], popup=plate, fill=False,  color=\"#58508d\").add_to(tectonic_quake)\n",
        "        HeatMap(data=data[[\"Latitude\", \"Longitude\"]],hue=\"Magnitude\",min_opacity=0.5,radius=1,gradient=gradient).add_to(tectonic_quake)\n",
        "\n",
        "fig.add_child(tectonic_quake)\n",
        "fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd8EQuyI_ivP"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "import math\n",
        "\n",
        "def haversine_distance(coord1, coord2):\n",
        "    lat1, lon1 = coord1\n",
        "    lat2, lon2 = coord2\n",
        "    lat1_rad, lon1_rad = math.radians(lat1), math.radians(lon1)\n",
        "    lat2_rad, lon2_rad = math.radians(lat2), math.radians(lon2)\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    a = math.sin(dlat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon/2)**2\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
        "    earth_radius_km = 6371\n",
        "    distance = earth_radius_km * c\n",
        "    return distance\n",
        "\n",
        "your_given_latitude = 12.34\n",
        "your_given_longitude = 56.78\n",
        "given_coordinate = (your_given_latitude, your_given_longitude)\n",
        "\n",
        "tectonic_quake = folium.Map(tiles=\"cartodbpositron\", zoom_start=5)\n",
        "gradient = {.33: \"#58508d\", .66: \"#ef5675\", 1: \"#ffa600\"}\n",
        "plates = list(tectonic_plates[\"plate\"].unique())\n",
        "\n",
        "closest_fault_lines = []\n",
        "closest_earthquakes = []\n",
        "distance_to_fault_lines = float('inf')\n",
        "num_closest_lines = 4\n",
        "\n",
        "for plate in plates:\n",
        "    plate_vals = tectonic_plates[tectonic_plates[\"plate\"] == plate]\n",
        "    lats = plate_vals[\"lat\"].values\n",
        "    lons = plate_vals[\"lon\"].values\n",
        "    points = list(zip(lats, lons))\n",
        "    indexes = [None] + [i + 1 for i, x in enumerate(points) if i < len(points) - 1 and abs(x[1] - points[i + 1][1]) > 300] + [None]\n",
        "\n",
        "    for i in range(len(indexes) - 1):\n",
        "        folium.vector_layers.PolyLine(points[indexes[i]:indexes[i+1]], popup=plate, fill=False,  color=\"#58508d\").add_to(tectonic_quake)\n",
        "        HeatMap(data=data[[\"Latitude\", \"Longitude\"]],hue=\"Magnitude\",min_opacity=0.5,radius=1,gradient=gradient).add_to(tectonic_quake)\n",
        "\n",
        "        for quake_lat, quake_lon in points[indexes[i]:indexes[i+1]]:\n",
        "            distance = haversine_distance(given_coordinate, (quake_lat, quake_lon))\n",
        "            if distance < distance_to_fault_lines:\n",
        "                distance_to_fault_lines = distance\n",
        "                closest_earthquakes = [(quake_lat, quake_lon, distance)]\n",
        "            elif distance == distance_to_fault_lines:\n",
        "                closest_earthquakes.append((quake_lat, quake_lon, distance))\n",
        "\n",
        "        distance_to_plate = haversine_distance(given_coordinate, (plate_vals['lat'].iloc[0], plate_vals['lon'].iloc[0]))\n",
        "        closest_fault_lines.append((plate, distance_to_plate))\n",
        "\n",
        "closest_fault_lines.sort(key=lambda x: x[1])\n",
        "closest_fault_lines = closest_fault_lines[:num_closest_lines]\n",
        "closest_earthquakes.sort(key=lambda x: x[2])\n",
        "closest_earthquakes = closest_earthquakes[:num_closest_lines]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQrd6i-MTpxk"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "your_given_latitude = 12.34\n",
        "your_given_longitude = 56.78\n",
        "given_coordinate = (your_given_latitude, your_given_longitude)\n",
        "\n",
        "def haversine_distance(coord1, coord2):\n",
        "    lat1, lon1 = coord1\n",
        "    lat2, lon2 = coord2\n",
        "    lat1_rad, lon1_rad = math.radians(lat1), math.radians(lon1)\n",
        "    lat2_rad, lon2_rad = math.radians(lat2), math.radians(lon2)\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    a = math.sin(dlat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon/2)**2\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
        "    earth_radius_km = 6371\n",
        "    distance = earth_radius_km * c\n",
        "    return distance\n",
        "\n",
        "def find_closest_faults_quakes(row):\n",
        "    given_coordinate = (row['Latitude'], row['Longitude'])\n",
        "    closest_fault_lines = []\n",
        "    closest_earthquakes = []\n",
        "\n",
        "    for plate in plates:\n",
        "        plate_vals = tectonic_plates[tectonic_plates[\"plate\"] == plate]\n",
        "        distance_to_plate = haversine_distance(given_coordinate, (plate_vals['lat'].iloc[0], plate_vals['lon'].iloc[0]))\n",
        "        closest_fault_lines.append((plate, distance_to_plate))\n",
        "\n",
        "    closest_fault_lines.sort(key=lambda x: x[1])\n",
        "    closest_fault_lines = closest_fault_lines[:num_closest_lines]\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        distance = haversine_distance(given_coordinate, (row['Latitude'], row['Longitude']))\n",
        "        closest_earthquakes.append((row['Latitude'], row['Longitude'], distance))\n",
        "\n",
        "    closest_earthquakes.sort(key=lambda x: x[2])\n",
        "    closest_earthquakes = closest_earthquakes[:num_closest_lines]\n",
        "\n",
        "    return closest_fault_lines, closest_earthquakes\n",
        "\n",
        "closest_fault_lines_list = []\n",
        "closest_earthquakes_list = []\n",
        "\n",
        "for index, row in data.head(20).iterrows():\n",
        "    closest_fault_lines, closest_earthquakes = find_closest_faults_quakes(row)\n",
        "    closest_fault_lines_list.append(closest_fault_lines)\n",
        "    closest_earthquakes_list.append(closest_earthquakes)\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'Latitude': data.head(20)['Latitude'],\n",
        "    'Longitude': data.head(20)['Longitude'],\n",
        "    'Closest_Fault_Lines': closest_fault_lines_list,\n",
        "    'Closest_Earthquakes': closest_earthquakes_list\n",
        "})\n",
        "\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12lutrBmytFc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm import tqdm\n",
        "\n",
        "data = data.sort_values(by=\"Magnitude\", ascending=False)\n",
        "\n",
        "def haversine_distance(coord1, coord2):\n",
        "    lat1, lon1 = coord1\n",
        "    lat2, lon2 = coord2\n",
        "    lat1_rad, lon1_rad = math.radians(lat1), math.radians(lon1)\n",
        "    lat2_rad, lon2_rad = math.radians(lat2), math.radians(lon2)\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    a = math.sin(dlat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon/2)**2\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
        "    earth_radius_km = 6371\n",
        "    distance = earth_radius_km * c\n",
        "    return distance\n",
        "\n",
        "distances_to_fault = []\n",
        "magnitudes_list = []\n",
        "depths_list = []\n",
        "\n",
        "for index, quake_row in tqdm(data.head(500).iterrows(), total=500):\n",
        "    min_distance_to_fault = float('inf')\n",
        "    for plate in plates:\n",
        "        plate_vals = tectonic_plates[tectonic_plates[\"plate\"] == plate]\n",
        "        lats = plate_vals[\"lat\"].values\n",
        "        lons = plate_vals[\"lon\"].values\n",
        "        points = list(zip(lats, lons))\n",
        "        for i in range(len(points) - 1):\n",
        "            distance_to_fault = haversine_distance((quake_row[\"Latitude\"], quake_row[\"Longitude\"]), points[i])\n",
        "            if distance_to_fault < min_distance_to_fault:\n",
        "                min_distance_to_fault = distance_to_fault\n",
        "\n",
        "        distances_to_fault.append(min_distance_to_fault)\n",
        "        magnitudes_list.append(quake_row[\"Magnitude\"])\n",
        "        depths_list.append(quake_row[\"Depth\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-cFnIJgQ4gj"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[5,3])\n",
        "plt.scatter(distances_to_fault, magnitudes_list, alpha=0.7)\n",
        "plt.ylabel(\"Distance to Fault (km)\")\n",
        "plt.xlabel(\"Magnitude\")\n",
        "plt.grid(True)\n",
        "plt.xlim([200, 700])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDsRSAEXOWWR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "# Assuming you have already imported the necessary libraries and data\n",
        "\n",
        "model_data = pd.DataFrame({\"Distance_to_Fault\": distances_to_fault,\n",
        "                           \"Depth\": depths_list,\n",
        "                           \"Magnitude\": magnitudes_list})\n",
        "\n",
        "X = model_data[\"Distance_to_Fault\"]\n",
        "y_depth = model_data[\"Depth\"]\n",
        "y_magnitude = model_data[\"Magnitude\"]\n",
        "\n",
        "def negative_exponential_func(x, a, b, c):\n",
        "    return a * np.exp(-b * x) + c\n",
        "\n",
        "depth_params, depth_covariance = curve_fit(negative_exponential_func, X, y_depth)\n",
        "magnitude_params, magnitude_covariance = curve_fit(negative_exponential_func, X, y_magnitude)\n",
        "\n",
        "plt.figure(figsize=[5, 3])\n",
        "plt.scatter(distances_to_fault, depths_list, label=\"Depth\", alpha=0.7)\n",
        "plt.scatter(distances_to_fault, magnitudes_list, label=\"Magnitude\", alpha=0.7)\n",
        "plt.plot(distances_to_fault, negative_exponential_func(X, *depth_params), color='red', label='Depth Exponential Fit')\n",
        "plt.plot(distances_to_fault, negative_exponential_func(X, *magnitude_params), color='blue', label='Magnitude Exponential Fit')\n",
        "plt.xlabel(\"Distance to Fault (km)\")\n",
        "plt.ylabel(\"Depth / Magnitude\")\n",
        "plt.title(\"Depth and Magnitude based on Distance to Fault (<= 200 km)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHNpRfy5USpq"
      },
      "outputs": [],
      "source": [
        "model_data = pd.DataFrame({\"Distance_to_Fault\": distances_to_fault,\n",
        "                           \"Depth\": depths_list,\n",
        "                           \"Magnitude\": magnitudes_list})\n",
        "\n",
        "X = model_data[[\"Distance_to_Fault\"]]\n",
        "y_depth = model_data[\"Depth\"]\n",
        "y_magnitude = model_data[\"Magnitude\"]\n",
        "depth_model = LinearRegression()\n",
        "magnitude_model = LinearRegression()\n",
        "\n",
        "depth_model.fit(X, y_depth)\n",
        "magnitude_model.fit(X, y_magnitude)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8wZcBSWUq0E"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial import KDTree\n",
        "\n",
        "tree = KDTree(data[[\"Latitude\", \"Longitude\"]])\n",
        "\n",
        "def find_nearest_earthquakes(coord, k=5):\n",
        "    distance_to_nearest, index_of_nearest = tree.query(coord, k=k)\n",
        "    return data.iloc[index_of_nearest]\n",
        "\n",
        "coordinate_to_search = (37, 37)\n",
        "nearest_earthquakes = find_nearest_earthquakes(coordinate_to_search)\n",
        "\n",
        "nearest_earthquakes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBr_9px-V4-E"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial import KDTree\n",
        "\n",
        "tree = KDTree(data[[\"Latitude\", \"Longitude\"]])\n",
        "\n",
        "fault_line_tree = KDTree(tectonic_plates[[\"lat\", \"lon\"]])\n",
        "\n",
        "def find_nearest_fault_line(coord):\n",
        "    distance_to_nearest_fault, index_of_nearest_fault = fault_line_tree.query(coord)\n",
        "    nearest_fault_line = tectonic_plates.iloc[index_of_nearest_fault]\n",
        "    return nearest_fault_line\n",
        "\n",
        "def calculate_distance_to_fault(row):\n",
        "    earthquake_coordinate = (row[\"Latitude\"], row[\"Longitude\"])\n",
        "    nearest_fault_line = find_nearest_fault_line(earthquake_coordinate)\n",
        "    distance_to_fault = haversine_distance(earthquake_coordinate, (nearest_fault_line[\"lat\"], nearest_fault_line[\"lon\"]))\n",
        "    return distance_to_fault\n",
        "\n",
        "data[\"Distance_to_Fault\"] = data.apply(calculate_distance_to_fault, axis=1)\n",
        "\n",
        "magnitude_weight = 0.33\n",
        "depth_weight = 0.33\n",
        "distance_to_fault_weight = 0.33\n",
        "\n",
        "def get_danger_score(earthquake):\n",
        "    magnitude = earthquake[\"Magnitude\"]\n",
        "    depth = earthquake[\"Depth\"]\n",
        "    distance_to_fault = earthquake[\"Distance_to_Fault\"]\n",
        "    log_distance_to_fault = np.log(distance_to_fault + 1)\n",
        "    composite_score = (magnitude_weight * magnitude) + (depth_weight * depth) + (distance_to_fault_weight * log_distance_to_fault)\n",
        "    return composite_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWn4NIllcDDj"
      },
      "outputs": [],
      "source": [
        "data[\"Composite Score\"] = data.apply(get_danger_score, axis=1)\n",
        "subset = data[[\"Date\", \"Latitude\", \"Longitude\", \"Magnitude\", \"Depth\", \"Distance_to_Fault\", \"Composite Score\"]]\n",
        "subset = subset.sort_values(by=\"Composite Score\", ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl6wiJzHN2aR"
      },
      "outputs": [],
      "source": [
        "demographics = pd.read_csv(\"datasets/socioeconomic.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "missing_values_columns_demographics = [col for col in demographics.columns if demographics[col].isnull().any()]\n",
        "demographics = demographics.drop(missing_values_columns_demographics, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFLb6sERiVCY"
      },
      "outputs": [],
      "source": [
        "demographics_2010 = demographics[demographics[\"year\"] == 2010]\n",
        "\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.scatter(demographics_2010[\"SES\"], demographics_2010[\"gdppc\"], alpha=0.7)\n",
        "plt.xlabel(\"Socioeconomic Score (SES)\")\n",
        "plt.ylabel(\"GDP per Capita (gpppc)\")\n",
        "plt.title(\"SES vs. GDP per Capita in 2010\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sl-YSR4jyvz"
      },
      "outputs": [],
      "source": [
        "!pip install reverse_geocode\n",
        "import reverse_geocode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qW8-9HTkiFJ"
      },
      "outputs": [],
      "source": [
        "tqdm.pandas()\n",
        "\n",
        "def get_ses_gdppc_popshare(row):\n",
        "    latitude = row[\"Latitude\"]\n",
        "    longitude = row[\"Longitude\"]\n",
        "\n",
        "    location = reverse_geocode.search([(latitude, longitude)])\n",
        "    country = location[0][\"country\"]\n",
        "\n",
        "    demographics_country_2010 = demographics[(demographics[\"country\"] == country) & (demographics[\"year\"] == 2010)]\n",
        "\n",
        "    if not demographics_country_2010.empty:\n",
        "        ses_value = demographics_country_2010[\"SES\"].iloc[0]\n",
        "        gdppc_value = demographics_country_2010[\"gdppc\"].iloc[0]\n",
        "        popshare_value = demographics_country_2010[\"popshare\"].iloc[0]\n",
        "        return pd.Series({\"SES\": ses_value, \"gdppc\": gdppc_value, \"popshare\": popshare_value})\n",
        "    else:\n",
        "        return pd.Series({\"SES\": None, \"gdppc\": None, \"popshare\": None})\n",
        "subset[[\"SES\", \"gdppc\", \"popshare\"]] = subset.progress_apply(get_ses_gdppc_popshare, axis=1)\n",
        "subset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehkkaS2jj99I"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "small = subset.groupby(\"gdppc\")[\"Composite Score\"].mean().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LngZ_Wlat_pJ"
      },
      "outputs": [],
      "source": [
        "economic = pd.read_csv(\"datasets/economicdata.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCjLvkYWltBO"
      },
      "outputs": [],
      "source": [
        "latitude_input = 37\n",
        "longitude_input = 37\n",
        "\n",
        "def locate_nearest_earthquakes(coord):\n",
        "    data[\"Distance_to_Fault\"] = data.apply(calculate_distance_to_fault, axis=1)\n",
        "    data[\"Distance_to_Input\"] = data.apply(lambda row: haversine_distance(coord, (row[\"Latitude\"], row[\"Longitude\"])), axis=1)\n",
        "    nearest_earthquakes = data.nsmallest(5, \"Distance_to_Input\")\n",
        "    nearest_earthquakes[\"Weight\"] = np.log(1 + nearest_earthquakes[\"Distance_to_Input\"])\n",
        "    nearest_earthquakes[\"Weighted_Danger_Level\"] = nearest_earthquakes[\"Weight\"] * nearest_earthquakes.apply(get_danger_score, axis=1)\n",
        "    weighted_composite_score = nearest_earthquakes[\"Weighted_Danger_Level\"].sum() / nearest_earthquakes[\"Weight\"].sum()\n",
        "    return weighted_composite_score\n",
        "\n",
        "input_coordinate = (latitude_input, longitude_input)\n",
        "\n",
        "weighted_composite_score = locate_nearest_earthquakes(input_coordinate)\n",
        "\n",
        "location = reverse_geocode.search([(latitude_input, longitude_input)])\n",
        "country_identified = location[0][\"country\"]\n",
        "\n",
        "demographics_country_2010 = demographics[(demographics[\"country\"] == country_identified) & (demographics[\"year\"] == 2010)]\n",
        "\n",
        "identified_extended = economic[economic[\"Countries\"] == country_identified]\n",
        "\n",
        "country_regulation = float(identified_extended[\"Regulation\"].iloc[0])\n",
        "country_regulatory_burden = float(identified_extended[\"Regulatory Burden\"].iloc[0])\n",
        "country_labor_market_regulations = float(identified_extended[\"Labor market regulations\"].iloc[0])\n",
        "country_licensing_restrictions = float(identified_extended[\"Licensing restrictions\"].iloc[0])\n",
        "country_business_regulations = float(identified_extended[\"Business regulations\"].iloc[0])\n",
        "country_administrative_requirements = float(identified_extended[\"Administrative requirements\"].iloc[0])\n",
        "\n",
        "if not demographics_country_2010.empty:\n",
        "    ses_value = demographics_country_2010[\"SES\"].iloc[0]\n",
        "    gdppc_value = demographics_country_2010[\"gdppc\"].iloc[0]\n",
        "else:\n",
        "    print(f\"Data not available for {country_identified} in the year 2010.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QivPBvjXznS9"
      },
      "outputs": [],
      "source": [
        "sum = country_regulation + country_regulatory_burden + country_labor_market_regulations + country_licensing_restrictions + country_business_regulations + country_administrative_requirements\n",
        "regulatory_score = round(sum/6, 3)\n",
        "print(f\"Regulatory Score: {regulatory_score}\")\n",
        "economic_score = round(ses_value/10, 3)\n",
        "print(f\"Economic Score: {economic_score}\")\n",
        "risk_score = round(weighted_composite_score, 3)\n",
        "print(f\"Risk Score: {risk_score}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
